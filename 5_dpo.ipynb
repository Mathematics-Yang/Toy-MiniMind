{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f31fb-4795-49cf-9a68-a13f8f7e5cae",
   "metadata": {},
   "source": [
    "# 5-DPO\n",
    "\n",
    "直接偏好优化（Direct Preference Optimization，DPO）是后训练阶段中，使用正反样例激励大模型产生符合人类偏好的回答的策略，为人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）提供了一个高效简化的替代方案。通过这一阶段的训练，大模型将会学会依照人类的喜好生成回复.\n",
    "\n",
    "在这个笔记本中，我们仅对 DPO 的训练流程进行展示和学习，因此只给出必要的代码片段，如 wandb 和 ddp 不会在此笔记本中涉及.\n",
    "\n",
    "此笔记本的完整实现见主仓库 `/minimind/train_dpo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e09f234-5b8b-4bfb-8d42-bb766a457acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model import MiniMindLM\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import DPODataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb946301-eb7e-45c6-9e1c-16cc0ee4db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c65d3-14f7-4569-bebd-cddb48f1851f",
   "metadata": {},
   "source": [
    "## 可选参数设置\n",
    "\n",
    "首先，查看训练的可选参数，这些参数在实际使用时通过命令行导入，为了保持笔记本的易用性，选择用 class 进行包装."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78b11a1-9189-4eaf-a094-07c0497c4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs: int = 5 # 训练轮数，延续 sft 基础上微调\n",
    "    batch_size: int = 2 # pretrain 数据集仅两个样本，设置 batch 为 2\n",
    "    # sft阶段学习率为 「5e-6」->「5e-7」长度512，建议离线正负样本「概率」偏好对齐阶段lr <=「1e-8」长度3000，否则很容易遗忘训坏\n",
    "    learning_rate: float = 5e-4 # 学习率\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' # 16 bit 浮点数：8 bit 指数 + 7 bit 尾数\n",
    "    # use_wandb: bool = False # 是否使用 wandb 我们不使用\n",
    "    wandb_project: str = 'MiniMind-Notebook'\n",
    "    num_workers: int = 1 # 工作进程数\n",
    "    # ddp：bool = False # 单机多卡\n",
    "    accumulation_steps: int = 1 # 梯度累积步数\n",
    "    grad_clip: float = 1.0 # 梯度剪裁\n",
    "    warmup_iters: int = 0 # 学习率热启动\n",
    "    log_interval: int = 1 # 每一步打印日志 仅用于观察\n",
    "    local_rank: int = 1 # device 设备号\n",
    "    dim: int = 512 # 词嵌入维度 模型超参数\n",
    "    n_layers: int = 1 # MiniMind Block 数量 模型超参数 | 由于 dpo 要加载两个模型 我们出于演示目的设定 n_layers = 1\n",
    "    max_seq_len: int = 512 # 序列长度阈值\n",
    "    use_moe: bool = False # 是否启用混合专家\n",
    "    data_path: str = './toydata/dpo_data.jsonl' # 数据集路径\n",
    "    save_dir: str = \"./output\"  # 模型保存目录\n",
    "    save_weight: str = \"minimind_dpo\"  # checkpoint 文件前缀\n",
    "    save_interval: int = 1  # 每多少步保存一次模型，0表示不保存 我们这里只展示训练过程（可选择的保存模型，建议先保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a72c8aa-ca60-4b0d-aabb-2c9d16b48175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看工作设备 cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'查看工作设备 {args.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb31ff-524b-4788-a25f-699edb887704",
   "metadata": {},
   "source": [
    "## 初始化训练\n",
    "\n",
    "接下来，我们对一些重要模块进行初始化，我们已经了解过，分词器，模型和数据集是大模型的基本组件，我们对其进行初始化.\n",
    "\n",
    "> 在这一阶段 我们调整的是大模型的问答偏好 因此与 sft 阶段同理 我们需要载入在 sft 阶段微调好的问答模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519b20d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "actor 模型 vs ref 参考模型：\n",
    "   - `model`（actor模型）：是后续训练的核心模型，参数可更新，用于学习优化（如RLHF中的策略模型）。\n",
    "   - `ref_model`（参考模型）：参数固定，用于和actor模型的输出对比，计算奖励/优势值，避免模型训练偏离SFT阶段的基础能力。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5971fc-4794-4903-b418-76f266bf19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "    # 初始化actor模型\n",
    "    model = MiniMindLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'./output/minimind_sft_{lm_config.dim}{moe_path}.pth' # 指示上一阶段训练保存的模型文件位置\n",
    "    state_dict = torch.load(ckp, map_location=args.device) # 载入模型状态字典\n",
    "    model.load_state_dict(state_dict, strict=False) # 装入模型\n",
    "    # 初始化参考模型\n",
    "    ref_model = MiniMindLM(lm_config)\n",
    "    ref_model.load_state_dict(state_dict, strict=False)\n",
    "    ref_model.eval()\n",
    "    ref_model.requires_grad_(False)\n",
    "\n",
    "    print(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    ref_model = ref_model.to(args.device)\n",
    "\n",
    "    return model, ref_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd7bd15-a060-4aba-8c5d-277705ec6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM总参数量：6.096 百万\n",
      "模型位于设备：cuda:0, 词表长度：6400, DataLoader：<torch.utils.data.dataloader.DataLoader object at 0x000001EEE57FE190>\n"
     ]
    }
   ],
   "source": [
    "lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)\n",
    "model, ref_model ,tokenizer = init_model(lm_config)\n",
    "\n",
    "# 构建数据集和数据加载器\n",
    "train_ds = DPODataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,  # 锁页内存，加速数据传输\n",
    "    drop_last=False,  # 是否丢弃最后一个不完整的批次\n",
    "    shuffle=False,  # DPO 训练不需要打乱数据 \n",
    "    num_workers=args.num_workers,  # 工作进程数，0表示在主进程中加载数据\n",
    ")\n",
    "\n",
    "print(f'模型位于设备：{model.device}, 词表长度：{tokenizer.vocab_size}, DataLoader：{train_loader}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d807a2af-41e9-4115-b470-a05f6a04613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印一个 iter 的数据:\n",
      "{'x_chosen': tensor([[  1,  85, 736,  ...,   0,   0,   0],\n",
      "        [  1,  85, 736,  ...,   0,   0,   0]]), 'y_chosen': tensor([[ 85, 736, 201,  ...,   0,   0,   0],\n",
      "        [ 85, 736, 201,  ...,   0,   0,   0]]), 'mask_chosen': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'x_rejected': tensor([[  1,  85, 736,  ...,   0,   0,   0],\n",
      "        [  1,  85, 736,  ...,   0,   0,   0]]), 'y_rejected': tensor([[ 85, 736, 201,  ...,   0,   0,   0],\n",
      "        [ 85, 736, 201,  ...,   0,   0,   0]]), 'mask_rejected': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n",
      "\n",
      "数据集大小：2, DataLoader 大小：1\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_loader)\n",
    "print(f'打印一个 iter 的数据:\\n{next(loader)}\\n')\n",
    "print(f'数据集大小：{len(train_ds)}, DataLoader 大小：{len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ad506-ab12-4186-ad63-ada143a33d52",
   "metadata": {},
   "source": [
    "我们发现，train loader 的每一个 iter 都包含一个拥有六个键值对的字典，这是因为 train_dataset 每一次取数据都会返回:\n",
    "\n",
    "- chosen 样本 X: 包含 \\<bos> 在内的输入 content\n",
    "- chosen 标签 Y: 包含 \\<eos> 在内的输出 content\n",
    "- chosen 掩码 loss_mask: 指示需要计算损失的 token 位置\n",
    "- rejected 样本 X: 包含 \\<bos> 在内的输入 content\n",
    "- rejected 标签 Y: 包含 \\<eos> 在内的输出 content\n",
    "- rejected 掩码 loss_mask: 指示需要计算损失的 token 位置\n",
    "\n",
    "由于我们的数据集只有两条数据，而 batch size 设置为 2，因此我们的 dataloader 只有一个 iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32462eb9-7008-4389-a54b-47ef1c1c4732",
   "metadata": {},
   "source": [
    "# 启动训练\n",
    "\n",
    "训练一个深度学习模型，还涉及到了优化器，损失函数和学习率调度. 接下来，我们查看 MiniMind 训练部分的代码，并进行一轮简单的训练.\n",
    "\n",
    "> DPO 阶段涉及 DPO 损失函数涉及 因此与前两个阶段相比内容略有增加 不过整体流程与逻辑类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1752f117-33c0-4810-886f-fbd1b4d44c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备类型：cuda\n",
      "使用混精度训练，数据类型：torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 学习率调度方面 采用余弦退火学习率\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "# 优化器方面 选择 AdamW 优化器 并在混精度场景下创建 scaler 进行梯度缩放避免数值下溢\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype in ['float16', 'bfloat16']))  # 专门解决混合精度训练中的数值下溢问题\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)  # AdamW 优化器\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "print(f'设备类型：{device_type}')\n",
    "# 根据指定的数据类型设置混精度训练的 dtype，以下步骤为不可缺少的混精度训练准备工作\n",
    "if args.dtype == 'bfloat16':\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif args.dtype == 'float16':\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32  # 默认为 FP32\n",
    "print(f'使用混精度训练，数据类型：{amp_dtype}')\n",
    "# 在 cuda 上启动混精度训练，否则空白上下文\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type='cuda', dtype=amp_dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c3ac1-92bf-4eae-8d22-7fac4cfa69df",
   "metadata": {},
   "source": [
    "DPO 的原理是增加偏好样本的对数概率与减小非偏好样本响应的对数概率.\n",
    "\n",
    "该阶段引入 DPO 损失函数，通过计算选择样本和拒绝样本的对数比率，然后基于这些比率计算 DPO 损失，适用于偏好学习任务."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd48f2da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "DPO损失的数学公式：\n",
    "$$\\text{loss} = -\\log\\left(\\sigma\\left(\\beta \\cdot \\left( (\\log \\pi_{\\theta}(y_c|x) - \\log \\pi_{\\theta}(y_r|x)) - (\\log \\pi_{\\text{ref}}(y_c|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right)\\right)\\right)$$\n",
    "其中：\n",
    "- $\\sigma$ 是sigmoid函数\n",
    "- $\\beta$ 是温度系数\n",
    "- $y_c$ 是chosen回答，$y_r$ 是rejected回答\n",
    "- $\\pi_\\theta$ 是策略actor模型，$\\pi_{\\text{ref}}$ 是参考ref模型\n",
    "\n",
    "简单理解：\n",
    "- 如果策略模型认为chosen比rejected好（$\\pi_{\\theta}(y_c) > \\pi_{\\theta}(y_r)$），且优势超过参考模型，那么`logits`为正，`log_sigmoid`接近0，损失接近0；\n",
    "- 如果策略模型偏好错误，`logits`为负，`log_sigmoid`为负数，损失会变大，驱动模型调整参数。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c07f54a-496e-4b1d-9394-dd2c266ab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_log_probs(logits, labels):\n",
    "    # logits shape: (batch_size, seq_len, vocab_size)\n",
    "    # labels shape: (batch_size, seq_len)\n",
    "    # log_probs shape: (batch_size, seq_len)\n",
    "    log_probs = F.log_softmax(logits, dim=2)  # 得到每个 token 的 log 概率分布\n",
    "    # labels.unsqueeze(2)：把labels从(batch_size, seq_len)变成(batch_size, seq_len, 1)，增加维度匹配log_probs\n",
    "    # torch.gather：按dim=2维度，根据index（标签id）提取对应位置的值，结果shape=(batch_size, seq_len, 1)\n",
    "    # squeeze(-1)：去掉最后一个维度，变回(batch_size, seq_len)\n",
    "    # 这一步的目的是从每个 token 的 log 概率分布中提取出对应标签的 log 概率值，得到每个 token 的 log 概率\n",
    "    log_probs_per_token = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)\n",
    "    return log_probs_per_token\n",
    "\n",
    "# DPO 损失函数\n",
    "def dpo_loss(ref_log_probs, policy_log_probs, mask, beta):\n",
    "    # ref_log_probs,policy_log_probs和mask的 shape: (batch_size, seq_len)\n",
    "    seq_lengths = mask.sum(dim=1, keepdim=True).clamp_min(1e-8)  # 计算每个序列的有效长度，防止零长度mask导致除零NaN\n",
    "    # 计算参考模型每个序列的平均 log 概率，shape=(batch_size,)\n",
    "    ref_log_probs = (ref_log_probs * mask).sum(dim=1) / seq_lengths.squeeze()  \n",
    "    # 计算策略模型每个序列的平均 log 概率，shape=(batch_size,)\n",
    "    policy_log_probs = (policy_log_probs * mask).sum(dim=1) / seq_lengths.squeeze() \n",
    "\n",
    "    # 拆分chosen和rejected样本（DPO数据格式要求：前半是chosen，后半是rejected）\n",
    "    batch_size = ref_log_probs.shape[0]\n",
    "    chosen_ref_log_probs = ref_log_probs[:batch_size // 2]\n",
    "    reject_ref_log_probs = ref_log_probs[batch_size // 2:]\n",
    "    chosen_policy_log_probs = policy_log_probs[:batch_size // 2]\n",
    "    reject_policy_log_probs = policy_log_probs[batch_size // 2:]\n",
    "\n",
    "    pi_logratios = chosen_policy_log_probs - reject_policy_log_probs  # 策略模型对chosen的偏好 - 对rejected的偏好（越大越好）\n",
    "    ref_logratios = chosen_ref_log_probs - reject_ref_log_probs  # 参考模型对chosen的偏好 - 对rejected的偏好（基准）\n",
    "    logits = pi_logratios - ref_logratios  # 策略模型相对参考模型的偏好优势（目标：让这个值越大越好）\n",
    "    loss = -F.logsigmoid(beta * logits)  # 维度为 (batch_size//2,) \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743562c-6e34-488d-bd84-f62eaee0ae20",
   "metadata": {},
   "source": [
    "接下来，我们来看看 MiniMind 的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb26958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, ref_model, lm_config, start_step=0, wandb=None, beta=0.1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step, batch in enumerate(loader, start=start_step + 1):\n",
    "        # 将数据移动到指定设备\n",
    "        x_chosen = batch['x_chosen'].to(args.device)\n",
    "        x_rejected = batch['x_rejected'].to(args.device)\n",
    "        y_chosen = batch['y_chosen'].to(args.device)\n",
    "        y_rejected = batch['y_rejected'].to(args.device)\n",
    "        mask_chosen = batch['mask_chosen'].to(args.device)\n",
    "        mask_rejected = batch['mask_rejected'].to(args.device)\n",
    "        # 将 chosen 和 rejected 数据合并，形成一个批次进行模型前向计算\n",
    "        x = torch.cat([x_chosen, x_rejected], dim=0)\n",
    "        y = torch.cat([y_chosen, y_rejected], dim=0)\n",
    "        mask = torch.cat([mask_chosen, mask_rejected], dim=0)\n",
    "\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            # 参考模型不计算梯度\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = ref_model(x)  # 参考模型前向计算，得到 logits\n",
    "                ref_logits = ref_outputs.logits\n",
    "            ref_log_probs = logits_to_log_probs(ref_logits, y)  # 计算参考模型的 log 概率\n",
    "            # 策略模型需要计算梯度\n",
    "            outputs = model(x)\n",
    "            logits = outputs.logits\n",
    "            policy_log_probs = logits_to_log_probs(logits, y)  # 计算策略模型的 log 概率\n",
    "            \n",
    "            dpo_loss_val = dpo_loss(ref_log_probs, policy_log_probs, mask, beta=beta)\n",
    "            loss = dpo_loss_val + outputs.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time \n",
    "            current_loss = loss.item() * args.accumulation_steps  # 恢复到未缩放的损失值\n",
    "            current_dpo_loss = dpo_loss_val.item()  # DPO 损失值\n",
    "            current_aux_loss = outputs.aux_loss  # 额外损失值\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            \n",
    "            print(\n",
    "                f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), '\n",
    "                f'loss: {current_loss:.4f}, '\n",
    "                f'dpo_loss: {current_dpo_loss:.4f}, '\n",
    "                f'aux_loss: {current_aux_loss:.4f}, '\n",
    "                f'learning_rate: {current_lr:.8f}, '\n",
    "                f'epoch_time: {eta_min:.3f}min'\n",
    "            )\n",
    "            \n",
    "            if wandb: \n",
    "                wandb.log({\n",
    "                    \"loss\": current_loss, \n",
    "                    \"dpo_loss\": current_dpo_loss, \n",
    "                    \"aux_loss\": current_aux_loss, \n",
    "                    \"learning_rate\": current_lr, \n",
    "                    \"epoch_time\": eta_min\n",
    "                })\n",
    "\n",
    "        # 到达指定保存步数时，保存模型（仅主进程）\n",
    "        if args.save_interval > 0 and (step % args.save_interval == 0 or step == iters - 1):\n",
    "            if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "                os.makedirs(args.save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "                model.eval()\n",
    "                moe_suffix = '_moe' if lm_config.use_moe else ''\n",
    "                ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.dim}{moe_suffix}.pth'\n",
    "                raw_model = model.module if isinstance(model, DistributedDataParallel) else model\n",
    "                raw_model = getattr(raw_model, '_orig_mod', raw_model)\n",
    "                state_dict = raw_model.state_dict()\n",
    "                torch.save({k: v.half().cpu() for k, v in state_dict.items()}, ckp)\n",
    "                print(f'模型已保存至：{ckp}')\n",
    "                model.train()\n",
    "                del state_dict\n",
    "\n",
    "        del x_chosen, x_rejected, y_chosen, y_rejected, mask_chosen, mask_rejected, x, y, mask\n",
    "        del ref_outputs, ref_logits, ref_log_probs, outputs, logits, policy_log_probs, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49386dc7-2919-428a-a073-f37d576afbcf",
   "metadata": {},
   "source": [
    "准备完毕，我们尝试一轮长度 1 个 iter 的训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac86572-ee04-43b5-b5fa-b1c65cbaebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5](1/1), loss: 0.6931, dpo_loss: 0.6931, aux_loss: 0.0000, learning_rate: 0.00050225, epoch_time: 0.000min\n",
      "模型已保存至：./output/minimind_dpo_512.pth\n",
      "Epoch:[2/5](1/1), loss: 0.5893, dpo_loss: 0.5893, aux_loss: 0.0000, learning_rate: 0.00037725, epoch_time: 0.000min\n",
      "模型已保存至：./output/minimind_dpo_512.pth\n",
      "Epoch:[3/5](1/1), loss: 0.5061, dpo_loss: 0.5061, aux_loss: 0.0000, learning_rate: 0.00022275, epoch_time: 0.000min\n",
      "模型已保存至：./output/minimind_dpo_512.pth\n",
      "Epoch:[4/5](1/1), loss: 0.4426, dpo_loss: 0.4426, aux_loss: 0.0000, learning_rate: 0.00009775, epoch_time: 0.000min\n",
      "模型已保存至：./output/minimind_dpo_512.pth\n",
      "Epoch:[5/5](1/1), loss: 0.4131, dpo_loss: 0.4131, aux_loss: 0.0000, learning_rate: 0.00005000, epoch_time: 0.000min\n",
      "模型已保存至：./output/minimind_dpo_512.pth\n",
      "dpo训练完成！\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, train_loader, iter_per_epoch, ref_model, lm_config)\n",
    "print('dpo训练完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5215ef67-2647-4519-966b-074c46c225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
