{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba635c1-cdcd-4d00-a123-00045edbd10f",
   "metadata": {},
   "source": [
    "# 3-Pretrain\n",
    "\n",
    "预训练是模型经历的第一个阶段，在该阶段，模型将会吸收知识，学习尽可能正确的下一词语预测范式。在这个笔记本中，我们仅对预训练的训练流程进行展示和学习，因此只给出必要的代码片段，如 wandb 和 ddp 不会在此笔记本中涉及。\n",
    "\n",
    "此笔记本的完整实现见主仓库 `/minimind/train_pretrain.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98cb706f-2e03-4d8e-8e45-3663c9ab3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from model.model import MiniMindLM\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import PretrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b6f015-0e5c-4e88-bb0d-32092993933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c9b5b-12eb-4cad-a859-c5101cdcba82",
   "metadata": {},
   "source": [
    "## 可选参数设置\n",
    "\n",
    "首先，查看训练的可选参数，这些参数在实际使用时通过解析命令行进行导入，我们用 class 进行包装."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f07e381-961e-4a4f-9f37-ff2f0bc2918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs: int = 5 # 训练轮数 只做预训练实验 设置为 5 轮\n",
    "    batch_size: int = 2 # pretrain 数据集仅两个样本，设置 batch 为 2\n",
    "    learning_rate: float = 5e-4 # 学习率\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' # 16 bit 浮点数：8 bit 指数 + 7 bit 尾数\n",
    "    # use_wandb: bool = False # 是否使用 wandb 我们不使用\n",
    "    wandb_project: str = 'MiniMind-Notebook'\n",
    "    num_workers: int = 1 # 工作进程数\n",
    "    # ddp：bool = False # 单机多卡\n",
    "    accumulation_steps: int = 1 # 梯度累积步数，用途是在显存受限的情况下模拟更大的 batch size\n",
    "    grad_clip: float = 1.0 # 梯度剪裁\n",
    "    warmup_iters: int = 0 # 学习率热启动\n",
    "    log_interval: int = 1 # 每一步打印日志 仅用于观察\n",
    "    local_rank: int = 1 # device 设备号\n",
    "    dim: int = 512 # 词嵌入维度 模型超参数\n",
    "    n_layers: int = 2 # MiniMind Block 数量 模型超参数\n",
    "    max_seq_len: int = 512 # 序列长度阈值\n",
    "    use_moe: bool = False # 是否启用混合专家\n",
    "    data_path: str = './toydata/pretrain_data.jsonl' # 数据集路径\n",
    "    save_dir: str = \"./output\"  # 模型保存目录\n",
    "    save_weight: str = \"minimind_pretrain\"  # checkpoint 文件前缀\n",
    "    save_interval: int = 1  # 每多少步保存一次模型，0表示不保存 我们这里只展示训练过程 （可选择的保存模型，建议先保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f73286e-48a0-4a5b-b7c4-8c5fefea5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看工作设备 cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'查看工作设备 {args.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b0ea8-0f41-47fe-8125-93629d03d451",
   "metadata": {},
   "source": [
    "## 初始化训练\n",
    "\n",
    "\n",
    "接下来，我们对一些重要模块进行初始化，我们已经了解过，分词器，模型和数据集是大模型的基本组件，我们对其进行初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9818e376-7f1e-4347-b8c3-c8e30f7d587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "    model = MiniMindLM(lm_config).to(args.device)  # 初始化模型并移动到指定设备\n",
    "    print(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d15001-b670-4333-9fbe-b224ab1993c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM总参数量：8.915 百万\n",
      "模型位于设备：cuda:0, 词表长度：6400, DataLoader：<torch.utils.data.dataloader.DataLoader object at 0x0000011847CD0CD0>\n"
     ]
    }
   ],
   "source": [
    "lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)\n",
    "model, tokenizer = init_model(lm_config)  # 初始化模型和分词器\n",
    "\n",
    "# 创建预训练数据集实例\n",
    "# 得到的结果是id列表，长度不超过 max_seq_len，超过部分会被截断\n",
    "train_ds = PretrainDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,  # 提高数据传输效率\n",
    "    drop_last=False,  # 如果数据集大小不能被 batch_size 整除，丢弃最后一个不完整的 batch\n",
    "    shuffle=False,  # 预训练数据集较小，不进行 shuffle\n",
    "    num_workers=args.num_workers,  # 使用的子进程数\n",
    ")\n",
    "\n",
    "print(f'模型位于设备：{model.device}, 词表长度：{tokenizer.vocab_size}, DataLoader：{train_loader}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c8c970-f448-4ea0-b21b-8f929e03f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印一个 iter 的数据:\n",
      "[tensor([[   1,   46,   46,  ...,    0,    0,    0],\n",
      "        [   1, 5349, 1619,  ...,    0,    0,    0]]), tensor([[  46,   46,   47,  ...,    0,    0,    0],\n",
      "        [5349, 1619, 2875,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])]\n",
      "\n",
      "数据集大小：2, DataLoader 大小：1\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_loader) \n",
    "print(f'打印一个 iter 的数据:\\n{next(loader)}\\n')\n",
    "print(f'数据集大小：{len(train_ds)}, DataLoader 大小：{len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91904d-7ddf-4105-a5f5-65b84315a73b",
   "metadata": {},
   "source": [
    "我们发现，train loader 的每一个 iter 都包含一个长度为 3 的张量列表，这是因为 train_dataset 每一次取数据都会返回三个张量，分别为:\n",
    "\n",
    "- 样本 X: 包含 \\<bos> 在内的输入 content\n",
    "- 标签 Y: 包含 \\<eos> 在内的输出 content\n",
    "- 掩码 loss_mask: 指示需要计算损失的 token 位置\n",
    "\n",
    "由于我们的数据集只有两条数据，而 batch size 设置为 2，因此我们的 dataloader 只有一个 iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40764309-9c98-45d5-ada5-e31c1aaa02c5",
   "metadata": {},
   "source": [
    "## 启动训练\n",
    "\n",
    "训练一个深度学习模型，还涉及到了优化器，损失函数和学习率调度。接下来，我们查看 MiniMind 训练部分的代码，并进行一轮简单的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739df18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 学习率\n",
    "采用余弦退火学习率调度，即余弦函数平滑衰减 + 固定下限保护，公式如下：\n",
    "$$\n",
    "\\text{lr}_{\\text{current}} = \\frac{\\text{lr}}{10} + 0.5 \\cdot \\text{lr} \\cdot \\left(1 + \\cos\\left(\\pi \\cdot \\frac{\\text{current\\_step}}{\\text{total\\_steps}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "#### AdamW 更新公式\n",
    "1. 矩更新：\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t,\\quad v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n",
    "\n",
    "2. 偏差修正：\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t},\\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "\n",
    "3. 核心参数更新：\n",
    "$$\\theta_t = \\theta_{t-1}(1-\\alpha\\lambda) - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f70b9cf-fcff-4242-bd96-ce34a5da55e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备类型：cuda\n",
      "使用混精度训练，数据类型：torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 学习率调度方面 采用余弦退火学习率\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "# 优化器方面 选择 AdamW 优化器 并在混精度场景下创建 scaler 进行梯度缩放避免数值下溢\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype in ['float16', 'bfloat16']))  # 专门解决混合精度训练中的数值下溢问题\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)  # AdamW 优化器\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "print(f'设备类型：{device_type}')\n",
    "# 根据指定的数据类型设置混精度训练的 dtype，以下步骤为不可缺少的混精度训练准备工作\n",
    "if args.dtype == 'bfloat16':\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif args.dtype == 'float16':\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32  # 默认为 FP32\n",
    "print(f'使用混精度训练，数据类型：{amp_dtype}')\n",
    "# 在 cuda 上启动混精度训练，否则空白上下文\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type='cuda', dtype=amp_dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101ffb4-1a97-4e29-8612-caf40a8aa05d",
   "metadata": {},
   "source": [
    "接下来，我们来看看 MiniMind 的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5b27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, start_step=0, wandb=None):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        # 将输入数据移动到指定设备\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        # 更新学习率\n",
    "        current_total_step = epoch * iters + step  # 当前训练步数\n",
    "        total_training_steps = args.epochs * iters  # 总训练步数\n",
    "        lr = get_lr(current_total_step, total_training_steps, args.learning_rate)\n",
    "        # lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate) 更新学习率的简化版本\n",
    "       \n",
    "        # 将更新后的学习率应用到优化器的参数组中\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # 前向推理和损失计算\n",
    "        with autocast_ctx:\n",
    "            res = model(input_ids=X)  # 前向推理，输入为 token ids，输出包含logits，aux_loss和past_key_values\n",
    "            logits = res.logits  # 模型输出的 logits 得分，形状为 (batch_size, seq_len, vocab_size)\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')  # reduction='none' 表示不进行平均或求和，保留每个元素的损失值\n",
    "            # 调整logits和labels形状以匹配交叉熵输入\n",
    "            shift_logits = logits.reshape(-1, logits.size(-1))  # 重塑为 (batch_size * seq_len, vocab_size)\n",
    "            shift_labels = Y.reshape(-1)  # 重塑为 (batch_size * seq_len)\n",
    "            shift_loss_mask = loss_mask.reshape(-1)  # 重塑为 (batch_size * seq_len)\n",
    "            # 计算基础损失\n",
    "            raw_loss = loss_fct(shift_logits, shift_labels)  # 计算每个元素的交叉熵损失，形状为 (batch_size * seq_len)\n",
    "            # 应用loss_mask，只计算需要监督的位置\n",
    "            masked_loss = (raw_loss * shift_loss_mask).sum() / (shift_loss_mask.sum() + 1e-8)\n",
    "            # 加上moe的辅助损失（若无moe则aux_loss为0）\n",
    "            total_loss = masked_loss + (res.aux_loss if res.aux_loss is not None else 0.0)\n",
    "            # 梯度累积：损失除以累积步数  相当于在显存受限的情况下模拟更大的 batch size\n",
    "            total_loss = total_loss / args.accumulation_steps\n",
    "\n",
    "        # 反向传播\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)  # 梯度剪裁\n",
    "            # 更新参数并调整缩放器\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad(set_to_none=True)  # set_to_none=True 可以更高效地清零梯度\n",
    "\n",
    "        # 日志记录\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time  # 计算已用时间\n",
    "            current_loss = total_loss.item() * args.accumulation_steps  # 还原为未除以累积步数的损失值\n",
    "            current_aux_loss = res.aux_loss if res.aux_loss is not None else 0.0\n",
    "            current_logits_loss = current_loss - current_aux_loss  # 计算仅语言模型部分的损失\n",
    "            current_lr = optimizer.param_groups[-1]['lr']  # 获取当前学习率\n",
    "            # 计算剩余时间\n",
    "            eta_seconds = (spend_time / step) * (iters - step) if step > 0 else 0\n",
    "            eta_min = eta_seconds // 60\n",
    "            print(\n",
    "                f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), '\n",
    "                f'loss: {current_loss:.4f}, '\n",
    "                f'logits_loss: {current_logits_loss:.4f}, '\n",
    "                f'aux_loss: {current_aux_loss:.4f}, '\n",
    "                f'lr: {current_lr:.8f}, '\n",
    "                f'epoch_time: {eta_min:.1f}min'\n",
    "            )\n",
    "            # wandb日志（若启用）\n",
    "            if wandb:\n",
    "                wandb.log({\n",
    "                    \"loss\": current_loss, \n",
    "                    \"logits_loss\": current_logits_loss, \n",
    "                    \"aux_loss\": current_aux_loss, \n",
    "                    \"learning_rate\": current_lr, \n",
    "                    \"epoch_time\": eta_min\n",
    "                })\n",
    "\n",
    "        # 到达指定保存步数时，保存模型（仅主进程）\n",
    "        if args.save_interval > 0 and (step % args.save_interval == 0 or step == iters - 1):\n",
    "            if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "                os.makedirs(args.save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "                model.eval()\n",
    "                moe_suffix = '_moe' if lm_config.use_moe else ''  # 根据是否使用 MOE 添加后缀\n",
    "                ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.dim}{moe_suffix}.pth' # checkpoint 文件名包含模型维度、MOE 信息\n",
    "                # 处理DDP和compile包装的模型，获取原始模型\n",
    "                raw_model = model.module if isinstance(model, DistributedDataParallel) else model\n",
    "                raw_model = getattr(raw_model, '_orig_mod', raw_model)  # 获取原始模型，兼容DDP和compile包装\n",
    "                state_dict = raw_model.state_dict()\n",
    "                # 保存模型（转为FP16并移到CPU，节省空间）\n",
    "                torch.save({k: v.half().cpu() for k, v in state_dict.items()}, ckp)\n",
    "                print(f'模型已保存至：{ckp}')\n",
    "                model.train()\n",
    "                del state_dict\n",
    "\n",
    "        # 清理显存（可选）\n",
    "        del X, Y, loss_mask, res, total_loss, raw_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792bf9c-95a1-4c0d-bb53-fac2a1132ee0",
   "metadata": {},
   "source": [
    "准备完毕，我们尝试一轮长度 1 个 iter 的训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f51549-2009-4322-86c8-7aa783169e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5](1/1), loss: 8.9759, logits_loss: 8.9759, aux_loss: 0.0000, lr: 0.00050225, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_pretrain_512.pth\n",
      "Epoch:[2/5](1/1), loss: 8.0780, logits_loss: 8.0780, aux_loss: 0.0000, lr: 0.00037725, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_pretrain_512.pth\n",
      "Epoch:[3/5](1/1), loss: 7.4790, logits_loss: 7.4790, aux_loss: 0.0000, lr: 0.00022275, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_pretrain_512.pth\n",
      "Epoch:[4/5](1/1), loss: 7.1507, logits_loss: 7.1507, aux_loss: 0.0000, lr: 0.00009775, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_pretrain_512.pth\n",
      "Epoch:[5/5](1/1), loss: 6.9811, logits_loss: 6.9811, aux_loss: 0.0000, lr: 0.00005000, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_pretrain_512.pth\n",
      "预训练完成！\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)  # 每轮迭代次数等于 DataLoader 的长度\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, train_loader, iter_per_epoch)\n",
    "print('预训练完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f257b9e-3a1e-4686-9516-3f9eeb7d0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
