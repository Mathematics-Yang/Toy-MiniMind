{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22fb5a6",
   "metadata": {},
   "source": [
    "# 8-QLoRA\n",
    "\n",
    "大语言模型的量化低秩适应(QLoRA, Quantized Low-Rank Adaptation of Large Language Models) 是一项基于LoRA改进的大模型参数高效微调技术，其在继承LoRA参数高效优势的基础上，通过量化进一步降低显存占用，让消费级硬件也能完成超大参数量模型的微调。\n",
    "\n",
    "由于大模型参数量极大（如65B参数模型），即使使用LoRA微调，其全精度基座模型仍会占用大量显存，QLoRA 的工作原理是将预训练基座模型进行4-bit量化压缩并冻结，同时在模型中插入可训练的低秩分解矩阵A与B，仅训练这些低秩矩阵，这使得使用QLoRA进行训练的显存效率大幅提升，硬件门槛显著降低，且能保持与LoRA相近的微调性能。\n",
    "\n",
    "具体来说，QLoRA 冻结了经过4-bit量化后的预训练模型$W_q$的权重，并注入可训练的秩分解矩阵 A 与 B，在微调时，仅训练降维矩阵 A 和 升维矩阵 B，量化基座模型全程不参与更新，微调结束后，将 AB 与反量化后的原始基座权重 W 进行叠加，实现模型适配特定任务。\n",
    "\n",
    "![images](./images/qlora.png)\n",
    "\n",
    "其中，基座模型采用专为大模型权重正态分布设计的NF4（NormalFloat4）格式进行4-bit量化，同时通过双重量化进一步优化显存占用；低秩矩阵A用随机高斯分布进行初始化，用0矩阵初始化B，从而保证训练开始时旁路矩阵为0矩阵，不干扰量化基座模型的原始输出。\n",
    "\n",
    "具体来看，假设模型经过量化预训练主干的输出为$W_q x$（计算时动态反量化为FP16精度以保证性能），在QLoRA微调阶段，我们可以用如下形式对输出进行表示。\n",
    "\n",
    "$$h=W_q x + \\Delta Wx = W_q x + BA x=(W_q + BA)x$$\n",
    "\n",
    "其中, $B \\in \\mathbb{R}^{d \\times r},A \\in \\mathbb{R}^{r\\times k}$，r 为QLoRA低秩矩阵的维数，$r << min(d, k)$；$W_q$ 为4-bit量化后的预训练基座权重，其存储精度远低于LoRA所用的FP16/BF16精度，可大幅降低显存占用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from model.model import MiniMindLM\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import SFTDataset\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93015b",
   "metadata": {},
   "source": [
    "### 可选参数设置\n",
    "\n",
    "首先，查看训练的可选参数，这些参数在实际使用时通过解析命令行进行导入，我们用 class 进行包装."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c38a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs: int = 5 # 训练轮数，延续 pretrain 基础上微调\n",
    "    batch_size: int = 2 # pretrain 数据集仅两个样本，设置 batch 为 2\n",
    "    learning_rate: float = 5e-4 # 学习率\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' # 16 bit 浮点数：8 bit 指数 + 7 bit 尾数\n",
    "    # use_wandb: bool = False # 是否使用 wandb 我们不使用\n",
    "    wandb_project: str = 'MiniMind-Notebook'\n",
    "    num_workers: int = 1 # 工作进程数\n",
    "    # ddp：bool = False # 单机多卡\n",
    "    accumulation_steps: int = 1 # 梯度累积步数\n",
    "    grad_clip: float = 1.0 # 梯度剪裁\n",
    "    warmup_iters: int = 0 # 学习率热启动\n",
    "    log_interval: int = 1 # 每一步打印日志 仅用于观察\n",
    "    local_rank: int = 1 # device 设备号\n",
    "    dim: int = 512 # 词嵌入维度 模型超参数\n",
    "    n_layers: int = 2 # MiniMind Block 数量 模型超参数\n",
    "    max_seq_len: int = 512 # 序列长度阈值\n",
    "    use_moe: bool = False # 是否启用混合专家\n",
    "    data_path: str = './toydata/lora_data.jsonl' # 数据集路径\n",
    "    save_dir: str = \"./output\"  # 模型保存目录\n",
    "    save_weight: str = \"minimind_qlora\"  # checkpoint 文件前缀\n",
    "    save_interval: int = 1  # 每多少步保存一次模型，0表示不保存 我们这里只展示训练过程（可选择的保存模型，建议先保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d252793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看工作设备 cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'查看工作设备 {args.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4006aa",
   "metadata": {},
   "source": [
    "## QLoRA Adapter\n",
    "\n",
    "QLoRA 矩阵与 LoRA 矩阵结构相同，也是具有一个隐藏层的全连接网络，其挂接在主干网络边侧进行参数更新，只需调整初始化精度，适配 4-bit 计算即可，我们来看看 MiniMind 模型是如何在主干网络外部定义 LoRA 网络结构的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28c0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        # QLoRA矩阵用bfloat16精度存储以节省内存\n",
    "        self.A = nn.Linear(in_features, rank, bias=False, dtype=torch.bfloat16)\n",
    "        self.B = nn.Linear(rank, out_features, bias=False, dtype=torch.bfloat16)\n",
    "        self.A.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        self.B.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入适配4-bit计算精度\n",
    "        x = x.to(torch.bfloat16)\n",
    "        return self.B(self.A(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb00611",
   "metadata": {},
   "source": [
    "可以看到，QLoRA 的网络结构也非常简单直观，我们接下来定义一个方法，将 QLoRA 网络应用到 MiniMind 模型的特定线性层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a56945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 QLoRA 模块绑定到模型的全连接层上，注意此处还未进行训练，仅是结构上的绑定\n",
    "def apply_qlora(model, rank=16):\n",
    "    for name, module in model.named_modules():\n",
    "        # 检测 Linear4bit，且只对方阵层绑定\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            # 对于4bit层，用 out_features 和 in_features 判断\n",
    "            if module.out_features == module.in_features:\n",
    "                qlora = QLoRA(\n",
    "                    module.in_features, \n",
    "                    module.out_features, \n",
    "                    rank=rank\n",
    "                ).to(args.device)\n",
    "                \n",
    "                setattr(module, 'qlora', qlora)\n",
    "                original_forward = module.forward\n",
    "\n",
    "                def forward_with_qlora(x, layer1=original_forward, layer2=qlora):\n",
    "                    x = x.to(args.device)\n",
    "                    out1 = layer1(x).to(torch.bfloat16)\n",
    "                    out2 = layer2(x).to(torch.bfloat16)\n",
    "                    return (out1 + out2).to(torch.bfloat16)\n",
    "                    \n",
    "                module.forward = forward_with_qlora\n",
    "                print(f'apply qlora on module: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc02dc",
   "metadata": {},
   "source": [
    "同理LoRA，我们可以声明一个小模型，对于 QLoRA 的绑定进行测试."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6465a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = bnb.nn.Linear4bit(64, 512)\n",
    "        self.linear2 = bnb.nn.Linear4bit(512, 512)\n",
    "        self.linear3 = bnb.nn.Linear4bit(512, 64)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear3(self.linear2(self.linear1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5bd1d",
   "metadata": {},
   "source": [
    "按照 apply_qlora 的函数逻辑，QLoRA 模块会应用在主干网络中满足 input_feature == output_feature 的模块上."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e855a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply qlora on module: linear2\n",
      "TestModel(\n",
      "  (linear1): Linear4bit(in_features=64, out_features=512, bias=True)\n",
      "  (linear2): Linear4bit(\n",
      "    in_features=512, out_features=512, bias=True\n",
      "    (qlora): QLoRA(\n",
      "      (A): Linear(in_features=512, out_features=16, bias=False)\n",
      "      (B): Linear(in_features=16, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (linear3): Linear4bit(in_features=512, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_model = TestModel()\n",
    "apply_qlora(test_model)\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f3ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59209f8a",
   "metadata": {},
   "source": [
    "完成了 QLoRA 模块在主干网络特定模块的绑定后，我们便可以冻结主干网络参数进行微调了，不过，考虑到主干网络权重在训练过程中并不会做任何参数更新，我们可以只保存 QLoRA 模块的参数来节省内存，下面给出加载/保存 QLoRA 权重的方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74563349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qlora(model, path):\n",
    "    state_dict = torch.load(path, map_location=model.device)\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'qlora'):\n",
    "            qlora_state = {k.replace(f'{name}.qlora.', ''): v for k, v in state_dict.items() if f'{name}.qlora.' in k}\n",
    "            module.qlora.load_state_dict(qlora_state)\n",
    "\n",
    "def save_qlora(model, path):\n",
    "    state_dict = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'qlora'):\n",
    "            qlora_state = {f'{name}.qlora.{k}': v for k, v in module.qlora.state_dict().items()}\n",
    "            state_dict.update(qlora_state)\n",
    "    torch.save(state_dict, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3bea",
   "metadata": {},
   "source": [
    "## Fine-Tuning MiniMind with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e683733",
   "metadata": {},
   "source": [
    "接下来，我们对分词器、MiniMindLM 和数据迭代器执行初始化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c618dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "    \n",
    "    # 在 CPU 上加载模型（float32）\n",
    "    model = MiniMindLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'./output/minimind_sft_{lm_config.dim}{moe_path}.pth'\n",
    "    state_dict = torch.load(ckp, map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # 将 nn.Linear 替换为 bnb.nn.Linear4bit\n",
    "    replace_linear_with_4bit(model)\n",
    "    model = model.to(args.device)  # 移到 GPU 上\n",
    "    \n",
    "    # 冻结所有参数\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# 4-bit 替换函数，将模型中的 nn.Linear 替换为 bnb.nn.Linear4bit，并正确处理权重和 bias\n",
    "def replace_linear_with_4bit(model):\n",
    "    # 收集需要替换的模块\n",
    "    replace_list = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            replace_list.append((name, module))\n",
    "    \n",
    "    for name, module in replace_list:\n",
    "        # 解析父模块和属性名\n",
    "        parts = name.split('.')\n",
    "        parent = model\n",
    "        for part in parts[:-1]:\n",
    "            parent = getattr(parent, part)\n",
    "        attr_name = parts[-1]\n",
    "        \n",
    "        # 创建 Linear4bit 层\n",
    "        has_bias = module.bias is not None\n",
    "        quant_linear = Linear4bit(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            bias=has_bias,  # 保持原有的 bias 配置\n",
    "            quant_type='nf4',  # 4-bit 量化类型\n",
    "            compute_dtype=torch.bfloat16  # 4-bit 计算使用 bfloat16 精度\n",
    "        )\n",
    "        \n",
    "        # 用 Params4bit 正确包装权重，移到 CUDA 时自动量化\n",
    "        quant_linear.weight = Params4bit(\n",
    "            module.weight.data,  # 直接使用原始权重数据，移到 CUDA 时自动量化\n",
    "            requires_grad=False,  # 权重不更新\n",
    "            quant_type='nf4',  # 4-bit 量化类型\n",
    "            compress_statistics=True  # 双重量化\n",
    "        )\n",
    "        \n",
    "        # 如果原线性层有 bias，也将其复制到新的量化层中\n",
    "        if has_bias:\n",
    "            quant_linear.bias = nn.Parameter(module.bias.data, requires_grad=False)\n",
    "        \n",
    "        # 将新的量化层替换原有的线性层\n",
    "        setattr(parent, attr_name, quant_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306bc619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply qlora on module: layers.0.attention.wq\n",
      "apply qlora on module: layers.0.attention.wo\n",
      "apply qlora on module: layers.1.attention.wq\n",
      "apply qlora on module: layers.1.attention.wo\n"
     ]
    }
   ],
   "source": [
    "lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)\n",
    "model, tokenizer = init_model(lm_config)\n",
    "apply_qlora(model)\n",
    "\n",
    "train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018d72f",
   "metadata": {},
   "source": [
    "可以看到，QLoRA 模块挂接在 Attention Block 的 Query 与 Output 线性层上，下面查看 QLoRA 微调下可学习参数的占比："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7105b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 总参数量: 7801344\n",
      "QLoRA 参数量: 65536\n",
      "QLoRA 参数占比: 0.84%\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "qlora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'qlora' in name)\n",
    "print(f\"LLM 总参数量: {total_params}\")\n",
    "print(f\"QLoRA 参数量: {qlora_params_count}\")\n",
    "print(f\"QLoRA 参数占比: {qlora_params_count / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a4656",
   "metadata": {},
   "source": [
    "接下来，冻结 MiniMindLM 主干网络的参数，做好 QLoRA 微调准备."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eeb618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qlora' in name:\n",
    "        param.requires_grad = True\n",
    "        qlora_params.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a40ce",
   "metadata": {},
   "source": [
    "### 启动训练\n",
    "\n",
    "接下来，我们定义 MiniMind QLoRA 微调所使用的优化器，损失函数和学习率调度，并进行一轮简单的训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aab2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率调度\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "# QLoRA优化器：PagedAdamW（显存优化）\n",
    "optimizer = bnb.optim.PagedAdamW(\n",
    "    qlora_params,\n",
    "    lr=args.learning_rate,\n",
    "    betas=(0.9, 0.999),  # AdamW 的默认 beta 参数\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    optim_bits=8  # 8-bit 优化器，节省显存\n",
    ")\n",
    "\n",
    "# 混精度配置\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "if args.dtype == 'bfloat16':\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif args.dtype == 'float16':\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.bfloat16\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type='cuda', dtype=amp_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153bc99",
   "metadata": {},
   "source": [
    "接下来，我们来看训练函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f4f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, lora_params, start_step=0, wandb=None):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(input_ids=X)\n",
    "            logits = res.logits\n",
    "\n",
    "            # 对logits/labels/loss_mask做同步截断（去掉最后1位，避免预测未来token，与pretrain不同的地方）\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # [batch, seq_len-1, vocab]\n",
    "            shift_labels = Y[..., 1:].contiguous()      # [batch, seq_len-1]\n",
    "            shift_loss_mask = loss_mask[..., 1:].contiguous()# [batch, seq_len-1] \n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            raw_loss = loss_fct(\n",
    "                shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "                shift_labels.reshape(-1)\n",
    "            )\n",
    "            shift_loss_mask = shift_loss_mask.reshape(-1)\n",
    "            masked_loss = (raw_loss * shift_loss_mask).sum() / (shift_loss_mask.sum() + 1e-8)\n",
    "            total_loss = masked_loss + (res.aux_loss if res.aux_loss is not None else 0.0)\n",
    "            # 梯度累积：损失除以累积步数  相当于在显存受限的情况下模拟更大的 batch size\n",
    "            total_loss = total_loss / args.accumulation_steps\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_loss = total_loss.item() * args.accumulation_steps\n",
    "            current_aux_loss = res.aux_loss if res.aux_loss is not None else 0.0\n",
    "            current_logits_loss = current_loss - current_aux_loss\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            print(\n",
    "                f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), ' \n",
    "                f'loss: {current_loss:.4f}, '\n",
    "                f'logits_loss: {current_logits_loss:.4f}, '\n",
    "                f'aux_loss: {current_aux_loss:.4f}, '\n",
    "                f'lr: {current_lr:.8f}, '\n",
    "                f'epoch_time: {eta_min:.1f}min'\n",
    "            )\n",
    "            if wandb: \n",
    "                wandb.log({\n",
    "                    \"loss\": current_loss, \n",
    "                    \"logits_loss\": current_logits_loss, \n",
    "                    \"aux_loss\": current_aux_loss, \n",
    "                    \"learning_rate\": current_lr, \n",
    "                    \"epoch_time\": eta_min\n",
    "                })\n",
    "\n",
    "        if args.save_interval > 0 and (step % args.save_interval == 0 or step == iters - 1):\n",
    "            if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "                os.makedirs(args.save_dir, exist_ok=True)\n",
    "                model.eval()\n",
    "                lora_save_path = f'{args.save_dir}/{args.save_weight}_{lm_config.dim}.pth'\n",
    "                save_qlora(model, lora_save_path)\n",
    "                print(f'模型已保存至：{lora_save_path}')\n",
    "                model.train()\n",
    "\n",
    "        del X, Y, loss_mask, res, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d9a44",
   "metadata": {},
   "source": [
    "接下来，我们启动一个 Epoch 的训练进行观察."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3210259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5](1/1), loss: 8.4772, logits_loss: 8.4772, aux_loss: 0.0000, lr: 0.00050225, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_qlora_512.pth\n",
      "Epoch:[2/5](1/1), loss: 8.4673, logits_loss: 8.4673, aux_loss: 0.0000, lr: 0.00037725, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_qlora_512.pth\n",
      "Epoch:[3/5](1/1), loss: 8.4525, logits_loss: 8.4525, aux_loss: 0.0000, lr: 0.00022275, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_qlora_512.pth\n",
      "Epoch:[4/5](1/1), loss: 8.4407, logits_loss: 8.4407, aux_loss: 0.0000, lr: 0.00009775, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_qlora_512.pth\n",
      "Epoch:[5/5](1/1), loss: 8.4357, logits_loss: 8.4357, aux_loss: 0.0000, lr: 0.00005000, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_qlora_512.pth\n",
      "qlora训练完成！\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, train_loader, iter_per_epoch, qlora_params)\n",
    "print('qlora训练完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6495b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece89a72",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
