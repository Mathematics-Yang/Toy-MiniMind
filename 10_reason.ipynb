{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edfeb4e-03ea-406a-a345-6fe19d509c37",
   "metadata": {},
   "source": [
    "# 10-Reason\n",
    "\n",
    "参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果，因此，使用冷启动 SFT + GRPO 训练方法对小模型推理能力的作用有限.因此，MiniMind 项目作者使用推理数据集对 MiniMind 系列模型进行黑盒蒸馏来训练推理模型.\n",
    "\n",
    "使用的推理数据格式:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"conversations\": [\n",
    "        {\"role\": \"user\", \"content\": \"Q1?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think>T1</think>\\n<answer>A1</answer>\"},\n",
    "        {\"role\": \"user\", \"content\": \"Q2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think>T2</think>\\n<answer>A2</answer>\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "此笔记本的完整实现见主仓库 `/minimind/train_reason.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7eb789f-18d9-45c5-acab-ca3167950911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model import MiniMindLM\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import SFTDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cefe29-fdf0-44c7-86fa-97206bc18161",
   "metadata": {},
   "source": [
    "## 可选参数设置\n",
    "\n",
    "首先，查看训练的可选参数，这些参数在实际使用时通过解析命令行进行导入，我们用 class 进行包装."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22643ad2-fc7a-4efd-ac53-81c2bcc88cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs: int = 5 # 训练轮数\n",
    "    batch_size: int = 2 # pretrain 数据集仅两个样本，设置 batch 为 2\n",
    "    learning_rate: float = 5e-4 # 学习率\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' # 16 bit 浮点数：8 bit 指数 + 7 bit 尾数\n",
    "    # use_wandb: bool = False # 是否使用 wandb 我们不使用\n",
    "    wandb_project: str = 'MiniMind-Notebook'\n",
    "    num_workers: int = 1 # 工作进程数\n",
    "    # ddp：bool = False # 单机多卡\n",
    "    accumulation_steps: int = 1 # 梯度累积步数\n",
    "    grad_clip: float = 1.0 # 梯度剪裁\n",
    "    warmup_iters: int = 0 # 学习率热启动\n",
    "    log_interval: int = 1 # 每一步打印日志 仅用于观察\n",
    "    local_rank: int = 1 # device 设备号\n",
    "    dim: int = 512 # 词嵌入维度 模型超参数\n",
    "    n_layers: int = 2 # MiniMind Block 数量 模型超参数\n",
    "    max_seq_len: int = 512 # 序列长度阈值\n",
    "    use_moe: bool = False # 是否启用混合专家\n",
    "    data_path: str = './toydata/r1_data.jsonl' # 数据集路径\n",
    "    save_dir: str = \"./output\"  # 模型保存目录\n",
    "    save_weight: str = \"minimind_reasoning\"  # checkpoint 文件前缀\n",
    "    save_interval: int = 1  # 每多少步保存一次模型，0表示不保存 我们这里只展示训练过程（可选择的保存模型，建议先保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dc3bf2-215f-4fc0-b7ce-6b4a09b0f2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看工作设备 cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'查看工作设备 {args.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5bce8-a578-4e56-9dd5-9cb43321da0c",
   "metadata": {},
   "source": [
    "接下来，我们对分词器、MiniMind 学生模型以及数据迭代器执行初始化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db202500-e265-434e-b0a4-c352854de648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "    model = MiniMindLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    # 热启动\n",
    "    ckp = f'./output/minimind_dpo_{lm_config.dim}{moe_path}.pth'  \n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1948aa4-381d-485a-9b19-e0e566e0b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM总参数量：8.915 百万\n",
      "模型位于设备：cuda:0, 词表长度：6400, DataLoader：<torch.utils.data.dataloader.DataLoader object at 0x000002480320A290>\n"
     ]
    }
   ],
   "source": [
    "lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)\n",
    "model, tokenizer = init_model(lm_config)\n",
    "\n",
    "train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f'模型位于设备：{model.device}, 词表长度：{tokenizer.vocab_size}, DataLoader：{train_loader}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be67e55-0d1b-4c5d-9304-3b65d5b81c4d",
   "metadata": {},
   "source": [
    "## 启动训练\n",
    "\n",
    "接下来，我们定义 MiniMind LoRA 微调所使用的优化器，损失函数和学习率调度，并进行一轮简单的训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59084328-05e4-49d3-8459-54c549131511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备类型：cuda\n",
      "使用混精度训练，数据类型：torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 学习率调度方面 采用余弦退火学习率\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "# 优化器方面 选择 AdamW 优化器 并在混精度场景下创建 scaler 进行梯度缩放避免数值下溢\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype in ['float16', 'bfloat16']))  # 专门解决混合精度训练中的数值下溢问题\n",
    "# 优化学生模型参数\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "print(f'设备类型：{device_type}')\n",
    "# 根据指定的数据类型设置混精度训练的 dtype，以下步骤为不可缺少的混精度训练准备工作\n",
    "if args.dtype == 'bfloat16':\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif args.dtype == 'float16':\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32  # 默认为 FP32\n",
    "print(f'使用混精度训练，数据类型：{amp_dtype}')\n",
    "# 在 cuda 上启动混精度训练，否则空白上下文\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type='cuda', dtype=amp_dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a241f42-32ec-4070-9fda-1cb39860089f",
   "metadata": {},
   "source": [
    "接下来，我们来看看训练函数.\n",
    "\n",
    "蒸馏思考数据集的训练过程与 SFT 类似，区别在于模型生成序列中，思考标签位置的预测错误惩罚被放大."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517b89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, tokenizer, lm_config, start_step=0, wandb=None):\n",
    "    start_of_think_ids = tokenizer('<think>').input_ids\n",
    "    end_of_think_ids = tokenizer('</think>').input_ids\n",
    "    start_of_answer_ids = tokenizer('<answer>').input_ids\n",
    "    end_of_answer_ids = tokenizer('</answer>').input_ids\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        # 将输入数据移动到指定设备\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(X)\n",
    "            shift_logits = res.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = Y[..., 1:].contiguous()\n",
    "            shift_loss_mask = loss_mask[..., 1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).view(shift_labels.size())\n",
    "\n",
    "            # 对特殊标记位置的 loss 进行放大\n",
    "            sp_ids = torch.isin(shift_labels.view(-1),\n",
    "                                torch.tensor(start_of_think_ids + end_of_think_ids\n",
    "                                             + start_of_answer_ids + end_of_answer_ids\n",
    "                                             ).to(args.device))  # [batch_size*seq_len-1]\n",
    "            loss_mask_flat = shift_loss_mask.view(-1) # [batch_size*seq_len-1]\n",
    "            loss_mask_sum = loss_mask_flat.sum()  # 计算有效位置的数量\n",
    "            loss_mask_flat[sp_ids] = 10   # 将特殊标记位置的 loss 放大 10 倍\n",
    "            shift_loss_mask = loss_mask_flat.view(shift_labels.size())  # 恢复 loss_mask 的原始形状\n",
    "            logits_loss = (loss * shift_loss_mask).sum() / loss_mask_sum\n",
    "            loss = logits_loss + res.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_loss = loss.item() * args.accumulation_steps\n",
    "            current_aux_loss = res.aux_loss if res.aux_loss is not None else 0.0\n",
    "            current_logits_loss = logits_loss.item()\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            print(\n",
    "                f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), '\n",
    "                f'loss: {current_loss:.4f}, '\n",
    "                f'logits_loss: {current_logits_loss:.4f}, '\n",
    "                f'aux_loss: {current_aux_loss:.4f}, '\n",
    "                f'lr: {current_lr:.8f}, '\n",
    "                f'epoch_time: {eta_min:.1f}min'\n",
    "            )\n",
    "            if wandb: \n",
    "                wandb.log({\n",
    "                    \"loss\": current_loss, \n",
    "                    \"logits_loss\": current_logits_loss, \n",
    "                    \"aux_loss\": current_aux_loss, \n",
    "                    \"learning_rate\": current_lr, \n",
    "                    \"epoch_time\": eta_min\n",
    "                })\n",
    "\n",
    "        # 到达指定保存步数时，保存模型（仅主进程）\n",
    "        if args.save_interval > 0 and (step % args.save_interval == 0 or step == iters - 1):\n",
    "            if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "                os.makedirs(args.save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "                model.eval()\n",
    "                moe_suffix = '_moe' if lm_config.use_moe else ''\n",
    "                ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.dim}{moe_suffix}.pth'\n",
    "                raw_model = model.module if isinstance(model, DistributedDataParallel) else model\n",
    "                raw_model = getattr(raw_model, '_orig_mod', raw_model)\n",
    "                state_dict = raw_model.state_dict()\n",
    "                torch.save({k: v.half().cpu() for k, v in state_dict.items()}, ckp)\n",
    "                print(f'模型已保存至：{ckp}')\n",
    "                model.train()\n",
    "                del state_dict\n",
    "\n",
    "        del X, Y, loss_mask, loss, shift_logits, shift_labels, shift_loss_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5aaf7-feac-42d9-adfe-9a583f26879d",
   "metadata": {},
   "source": [
    "接下来，我们启动一个 Epoch 的训练进行观察."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f021611c-6291-4c50-acf0-9a24dde87e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5](1/1), loss: 11.7936, logits_loss: 11.7936, aux_loss: 0.0000, lr: 0.00050225, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_reasoning_512.pth\n",
      "Epoch:[2/5](1/1), loss: 10.2723, logits_loss: 10.2723, aux_loss: 0.0000, lr: 0.00037725, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_reasoning_512.pth\n",
      "Epoch:[3/5](1/1), loss: 9.3473, logits_loss: 9.3473, aux_loss: 0.0000, lr: 0.00022275, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_reasoning_512.pth\n",
      "Epoch:[4/5](1/1), loss: 8.8924, logits_loss: 8.8924, aux_loss: 0.0000, lr: 0.00009775, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_reasoning_512.pth\n",
      "Epoch:[5/5](1/1), loss: 8.6720, logits_loss: 8.6720, aux_loss: 0.0000, lr: 0.00005000, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_reasoning_512.pth\n",
      "reasoning 训练完成！\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, train_loader, iter_per_epoch, tokenizer, lm_config)\n",
    "print('reasoning 训练完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba06e90-680e-47ff-99e3-49fd91d74dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
