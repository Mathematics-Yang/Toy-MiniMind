{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70560a4d-ff57-4699-afb2-52f6a145b97e",
   "metadata": {},
   "source": [
    "# 4-SFT\n",
    "\n",
    "在预训练阶段后，我们应该能够获得一个下一词预测模型，此时的模型已经掌握了大量的知识。不过，仅仅具备下一词预测能力是不够的，我们希望大模型能够获得问答能力，这一能力便是在有监督微调（Supervised Fine Tuning，SFT）阶段获得的.\n",
    "\n",
    "在这个笔记本中，我们仅对 SFT 的训练流程进行展示和学习，因此只给出必要的代码片段，如 wandb 和 ddp 不会在此笔记本中涉及.\n",
    "\n",
    "此笔记本的完整实现见主仓库 `/minimind/train_full_sft.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2508c8c7-eb44-4ac0-918c-41479f41d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model import MiniMindLM\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import SFTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a79e140-23c7-4498-9a4d-ef7459692996",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1d40e-e074-4856-8c67-da8019f250ab",
   "metadata": {},
   "source": [
    "## 可选参数设置\n",
    "\n",
    "首先，查看训练的可选参数，这些参数在实际使用时通过命令行导入，为了保持笔记本的易用性，选择用 class 进行包装."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cbd6cf-081f-4c31-8dce-52566004801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs: int = 5 # 训练轮数，延续 pretrain 基础上微调\n",
    "    batch_size: int = 2 # pretrain 数据集仅两个样本，设置 batch 为 2\n",
    "    learning_rate: float = 5e-4 # 学习率\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' # 16 bit 浮点数：8 bit 指数 + 7 bit 尾数\n",
    "    # use_wandb: bool = False # 是否使用 wandb 我们不使用\n",
    "    wandb_project: str = 'MiniMind-Notebook'\n",
    "    num_workers: int = 1 # 工作进程数\n",
    "    # ddp：bool = False # 单机多卡\n",
    "    accumulation_steps: int = 1 # 梯度累积步数\n",
    "    grad_clip: float = 1.0 # 梯度剪裁\n",
    "    warmup_iters: int = 0 # 学习率热启动\n",
    "    log_interval: int = 1 # 每一步打印日志 仅用于观察\n",
    "    local_rank: int = 1 # device 设备号\n",
    "    dim: int = 512 # 词嵌入维度 模型超参数\n",
    "    n_layers: int = 2 # MiniMind Block 数量 模型超参数\n",
    "    max_seq_len: int = 512 # 序列长度阈值\n",
    "    use_moe: bool = False # 是否启用混合专家\n",
    "    data_path: str = './toydata/sft_data.jsonl' # 数据集路径\n",
    "    save_dir: str = \"./output\"  # 模型保存目录\n",
    "    save_weight: str = \"minimind_sft\"  # checkpoint 文件前缀\n",
    "    save_interval: int = 1  # 每多少步保存一次模型，0表示不保存 我们这里只展示训练过程（可选择的保存模型，建议先保存）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fadb6-f78f-45a0-ad3e-d41d7253c72b",
   "metadata": {},
   "source": [
    "## 初始化训练\n",
    "\n",
    "接下来，我们对一些重要模块进行初始化，我们已经了解过，分词器，模型和数据集是大模型的基本组件，我们对其进行初始化.\n",
    "\n",
    "> 注意 与预训练阶段不同的是 在 sft 阶段 我们实际上是在上一阶段训练获得的模型的基础上修改数据集进行接续训练 因此需要载入上一阶段的模型权重 出于展示的目的 载入权重的代码在此笔记本中只作展示 并不执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64a0e37-a9d0-4ce6-b5ee-7b2728de879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "    model = MiniMindLM(lm_config).to(args.device)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'./output/minimind_pretrain_{lm_config.dim}{moe_path}.pth' # 指示上一阶段训练保存的模型文件位置\n",
    "    state_dict = torch.load(ckp, map_location=args.device) # 载入模型状态字典\n",
    "    model.load_state_dict(state_dict, strict=False) # 装入模型\n",
    "    print(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eec33f3-2ac2-4c75-b58e-844d89d8add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM总参数量：8.915 百万\n",
      "模型位于设备：cuda:0, 词表长度：6400, DataLoader：<torch.utils.data.dataloader.DataLoader object at 0x0000020E7FB04910>\n"
     ]
    }
   ],
   "source": [
    "lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)\n",
    "model, tokenizer = init_model(lm_config)\n",
    "\n",
    "# 准备数据集和数据加载器\n",
    "train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "print(f'模型位于设备：{model.device}, 词表长度：{tokenizer.vocab_size}, DataLoader：{train_loader}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02644e0-e61d-4df0-9203-fa841cd83635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印一个 iter 的数据:\n",
      "[tensor([[  1,  85, 736,  ...,   0,   0,   0],\n",
      "        [  1,  85, 736,  ...,   0,   0,   0]]), tensor([[ 85, 736, 201,  ...,   0,   0,   0],\n",
      "        [ 85, 736, 201,  ...,   0,   0,   0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])]\n",
      "\n",
      "数据集大小：2, DataLoader 大小：1\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_loader)\n",
    "print(f'打印一个 iter 的数据:\\n{next(loader)}\\n')\n",
    "print(f'数据集大小：{len(train_ds)}, DataLoader 大小：{len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac8a47-d26a-49da-82fb-468ef5ac91a4",
   "metadata": {},
   "source": [
    "我们发现，train loader 的每一个 iter 都包含一个长度为 3 的张量列表，这是因为 train_dataset 每一次取数据都会返回三个张量，分别为:\n",
    "\n",
    "- 样本 X: 包含 \\<bos> 在内的输入 conversation\n",
    "- 标签 Y: 包含 \\<eos> 在内的输出 conversation\n",
    "- 掩码 loss_mask: 指示需要计算损失的 token 位置\n",
    "\n",
    "由于我们的数据集只有两条数据，而 batch size 设置为 2，因此我们的 dataloader 只有一个 iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23f937-f333-4c02-a3b4-a768818c0e2b",
   "metadata": {},
   "source": [
    "## 启动训练\n",
    "\n",
    "训练一个深度学习模型，还涉及到了优化器，损失函数和学习率调度. 接下来，我们查看 MiniMind 训练部分的代码，并进行一轮简单的训练.\n",
    "\n",
    "> 不难发现 pretrain 阶段和 sft 阶段的训练主体差不多 因为这两个阶段的差异体现在数据集格式 而数据集在经过 chat template 格式化后差异小了很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32a59a-be0b-4426-b47e-c522322bef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备类型：cuda\n",
      "使用混精度训练，数据类型：torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 学习率调度方面 采用余弦退火学习率\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "\n",
    "# 优化器方面 选择 AdamW 优化器 并在混精度场景下创建 scaler 进行梯度缩放避免数值下溢\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype in ['float16', 'bfloat16']))  # 专门解决混合精度训练中的数值下溢问题\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)  # AdamW 优化器\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "print(f'设备类型：{device_type}')\n",
    "# 根据指定的数据类型设置混精度训练的 dtype，以下步骤为不可缺少的混精度训练准备工作\n",
    "if args.dtype == 'bfloat16':\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif args.dtype == 'float16':\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32  # 默认为 FP32\n",
    "print(f'使用混精度训练，数据类型：{amp_dtype}')\n",
    "# 在 cuda 上启动混精度训练，否则空白上下文\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type='cuda', dtype=amp_dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13963ec0-d22e-4120-b0f2-8f5e5abfcda1",
   "metadata": {},
   "source": [
    "接下来，我们来看看 MiniMind 的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, start_step=0, wandb=None):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        # 将输入数据移动到指定设备\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "\n",
    "        # 更新优化器的学习率\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(input_ids=X)  # 前向推理\n",
    "            logits = res.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "            # 对logits/labels/loss_mask做同步截断（去掉最后1位，避免预测未来token，与pretrain不同的地方）\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # [batch, seq_len-1, vocab]\n",
    "            shift_labels = Y[..., 1:].contiguous()      # [batch, seq_len-1]\n",
    "            shift_loss_mask = loss_mask[..., 1:].contiguous() # [batch, seq_len-1] \n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            raw_loss = loss_fct(\n",
    "                shift_logits.reshape(-1, shift_logits.size(-1)),  # [batch*(seq_len-1), vocab]\n",
    "                shift_labels.reshape(-1)                          # [batch*(seq_len-1)]\n",
    "            )\n",
    "            # 应用损失掩码\n",
    "            shift_loss_mask = shift_loss_mask.reshape(-1)  # [batch*(seq_len-1)]\n",
    "            masked_loss = (raw_loss * shift_loss_mask).sum() / (shift_loss_mask.sum() + 1e-8)\n",
    "            # 加上moe的辅助损失（若无moe则aux_loss为0）\n",
    "            total_loss = masked_loss + (res.aux_loss if res.aux_loss is not None else 0.0)\n",
    "            # 梯度累积：损失除以累积步数  相当于在显存受限的情况下模拟更大的 batch size\n",
    "            total_loss = total_loss / args.accumulation_steps\n",
    "            \n",
    "            # 梯度累积：损失归一化\n",
    "            total_loss = total_loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time  # 计算已用时间\n",
    "            current_loss = total_loss.item() * args.accumulation_steps  # 恢复实际损失值\n",
    "            current_aux_loss = res.aux_loss if res.aux_loss is not None else 0.0  # 辅助损失\n",
    "            current_logits_loss = current_loss - current_aux_loss  \n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            # 计算剩余时间\n",
    "            eta_seconds = (spend_time / step) * (iters - step) if step > 0 else 0\n",
    "            eta_min = eta_seconds // 60\n",
    "            print(\n",
    "                f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), '\n",
    "                f'loss: {current_loss:.4f}, logits_loss: {current_logits_loss:.4f}, '\n",
    "                f'aux_loss: {current_aux_loss:.4f}, '\n",
    "                f'lr: {current_lr:.8f}, '\n",
    "                f'epoch_time: {eta_min:.1f}min'\n",
    "            )\n",
    "            if wandb: \n",
    "                wandb.log({\n",
    "                    \"loss\": current_loss,\n",
    "                    \"logits_loss\": current_logits_loss, \n",
    "                    \"aux_loss\": current_aux_loss, \n",
    "                    \"learning_rate\": current_lr, \n",
    "                    \"epoch_time\": eta_min\n",
    "                })\n",
    "        \n",
    "        # 到达指定保存步数时，保存模型（仅主进程）\n",
    "        if args.save_interval > 0 and (step % args.save_interval == 0 or step == iters - 1):\n",
    "            if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "                os.makedirs(args.save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "                model.eval()\n",
    "                moe_suffix = '_moe' if lm_config.use_moe else ''\n",
    "                ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.dim}{moe_suffix}.pth'\n",
    "                raw_model = model.module if isinstance(model, DistributedDataParallel) else model\n",
    "                raw_model = getattr(raw_model, '_orig_mod', raw_model)\n",
    "                state_dict = raw_model.state_dict()\n",
    "                torch.save({k: v.half().cpu() for k, v in state_dict.items()}, ckp)\n",
    "                print(f'模型已保存至：{ckp}')\n",
    "                model.train()\n",
    "                del state_dict\n",
    "\n",
    "        del X, Y, loss_mask, res, total_loss, raw_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f28c2-418e-453c-88aa-695468f9216c",
   "metadata": {},
   "source": [
    "准备完毕，我们尝试一轮长度 1 个 iter 的训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341200c9-0d3a-4ed1-9dd2-d2fc35d02564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5](1/1), loss: 8.7212, logits_loss: 8.7212, aux_loss: 0.0000, lr: 0.00050225, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_sft_512.pth\n",
      "Epoch:[2/5](1/1), loss: 7.2350, logits_loss: 7.2350, aux_loss: 0.0000, lr: 0.00037725, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_sft_512.pth\n",
      "Epoch:[3/5](1/1), loss: 6.2444, logits_loss: 6.2444, aux_loss: 0.0000, lr: 0.00022275, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_sft_512.pth\n",
      "Epoch:[4/5](1/1), loss: 5.7528, logits_loss: 5.7528, aux_loss: 0.0000, lr: 0.00009775, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_sft_512.pth\n",
      "Epoch:[5/5](1/1), loss: 5.4913, logits_loss: 5.4913, aux_loss: 0.0000, lr: 0.00005000, epoch_time: 0.0min\n",
      "模型已保存至：./output/minimind_sft_512.pth\n",
      "sft训练完成！\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, train_loader, iter_per_epoch)\n",
    "print('sft训练完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0480212-1500-49fb-b679-7bdf2c100c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
